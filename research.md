'On Junitaki Falls' is a digital score characterized by the use of intelligent computational systems. There are several different approaches to the implementation of computational intelligence within the digital score with each offering differing degrees of felt co-operation, presence and autonomy. This composition explored the musicianship and creativity in digital scores employing artificial intelligence (A.I.). The musician at the centre of this exploration was Christopher Redgate , a musician-researcher who investigates the contemporary oboe. On Junitaki Falls is concerned with creating a thinking-machine counterpart that worked co-operatively with the musician within the flow of musicking. After a period of several months practice with this score, the pieces were performed at Tempo Reale  festival in Florence, Italy in December 2017, as part of the 30th anniversary of Berio’s studio.

On Junitaki Falls is a composition for live instrument and two artificial intelligent performers  controlled by a central computer system that also generates a visual score for the live performer. The visual score is generated from a fixed library of fragments of Roger Jannotta’s transcription of Eric Dolphy’s bass clarinet solo of God Bless The Child . These fragments were categorised into folders defined by the master chord sequence of the original song, e.g. all the transcribed notes played over the chord Bb7 are contained in a folder marked Bb7. 

At the start of each performance the system defines its own pathway through the folder structure thereby influencing the harmonic progression (chord sequence) of each performance. The computer meanders through this sequence in performance by randomly generating the duration of each chord in the sequence (relating to breath lengths between 12 and 80 seconds). Image extracts from the corresponding folders are meshed together on the screen in a continuously unfolding collage  (see Figure 5.14). These change when the computer moves onto the next chord in its sequence. This represents the visual portion of the score for the live performer who can choose to interpret the combination of the visual images in three distinct modes, and is free to choose from either of these in the flow of the performance. These are note - pure tone and single pitch; node - harmonic spectra in linear or density organisation (e.g. yellow tremolo as linear, or stacked harmonics/ multi phonics as densities); noise - inharmonic spectra such as white noise (breath) or pink noise (combination of inharmonic and harmonic spectra)
The source sounding material for the A.I. performers is captured audio of the human musician’s previous engagement with the composition. Each chord in the computer’s generative sequence is also linked to an audio folder structure that stores and replays the corresponding audio in the sequence. This operates as a memory bank of the previous times the live performer played over the images meshed from, say, the fragments in the Bb7 folder, but are manipulated in some way (backwards, slowed down to half speed, or quarter speed). The live response to this is in turn recorded back in the audio section of each corresponding folder as memories. Over the course of many years of engaging with On Junitaki Falls the recall of these memories will remind the live performer of previous thoughts and ideas, whilst drawing attention to the fact that they are recording memories for the future . Furthermore, that each performer will create their own unique version of the piece developed from their own aesthetic understanding of it over time.

Having both visual and sounding elements to the digital score enhanced his experience of performing On Junitaki Falls. This created a ‘multi-sense immersion: hearing, seeing, responding and perhaps surprisingly a tactile response.’ Although he had worked with this piece longer that the other two as part of the seeding and learning process, it felt ‘very fresh and new’ in the performance, like ‘seeing a favourite landscape from a new angle perhaps’. In the training sessions Redgate had ‘developed a language of sounds, responses, fingerings (including some very unorthodox fingerings)’ to the visual and sounding materials emitted from the score. But in the performance, these familiar elements were presented in a different context to him. Instead his perception of the A.I. elements inside an authentic performance environment (to an audience in a music festival) shifted in meaning and gave rise to new opportunities to explore and ‘new experiences of the work’.

Redgate felt that the multi-layered and nuanced responses with these scores led to ‘many levels of meaning’. He felt that there was no ‘bland moments but rather a constant involvement on many levels.’ Alongside the ‘emotional and hidden language of pitch and gesture’ evoked in On Junitaki Falls was a striking sense of ‘feeling and touch’ emanating from the digital score. He reflected that this sense of feeling was apparent in ‘reading the state of an audience or fellow musicians’ (e.g. the A.I. musicians); or a feeling of the ‘state of the development of a work at a particular point’ (e.g. the ongoing narrative evoked by the A.I.). For Redgate, touch was the sensation of ‘feedback’ from his interactions with the digital score. He clarified this by stating that ‘when responding to these scores the touch/ listening is very important because that is what tells [me] about pitch choices - it sounds strange but I tend to feel pitches and can play with them as I wish.’
