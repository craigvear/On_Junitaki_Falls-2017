

## Craig Vear
## On Junitaki Falls
## Trio for solo instrument and two A.I. performers
## (2016-17)
## Copyright Craig Vear 2017

### About the piece
Created for, and inspired by, Roger Heaton and Christopher Redgate
First performance:
Instrumentation: Duration:
Christopher Redgate, Tempo Reale, Florence, Italy, 2nd December 2017.
Any instrument
Open duration; default setting is 35 minutes

### Programme notes
On Junitaki Falls, is a composition for live instrument and two artificial intelligent performers controlled by a central computer system that also operates as the score. Although Junitaki Falls exists as a geological feature in Japan, the reference to it as the title of this piece is inspired by Haruki Murakami's book 'A Wild Sheep Chase'. In this book a smaller story exists within the larger novel, with its presence felt throughout the book. In the piece, a smaller song exists within the larger composition, and like the novel it alters the whole music through its constant presence, even if we never hear it as itself. This smaller song exists as fragments of manuscript from Roger Jannotta’s transcription of Eric Dolphy’s bass clarinet solo of God Bless the Child composed by Billie Holiday and Arthur Herzog, Jr. in 1939. Already this is the start of a complex matryoshka (“Russian”) doll of layered interpretation. Add to this the process each individual performer of this work must complete in order to grow their score: months of feeding the AI system with progressively abstract layers of interpretation. The end result is much like the cascading mini falls that make up the whole of Junitaki’s geological feature.

### Acknowledgements
The composer wishes to thank Roger Jannotta for allowing his amazing transcription of Eric Dolphy’s solo to become the source material for this composition. Also, this piece wouldn't have been created if it weren’t for Roger Heaton who inspired the musical language of this piece and initiated the commission, and Christopher Redgate who braved the frontier nature of the performance and feed the first beta version to its world premiere in Florence.

Dedicated to the three R’s: Christopher Redgate, Roger Heaton and Roger Jannotta

### Performers instructions
On Junitaki Falls is a composition for a trio of performers, consisting of one live performer, and two A.I. controlled performers created from a library of memories created during each rehearsal and performance with the composition.
On Junitaki Falls is not an out-of-the-box composition; it is an intelligent software environment that needs to learn from each performer through a process of prolonged interaction. This is completed using the following phases:

### Phase 1 – seeding the system
The software environment requires the performer to make two fundamental decisions that will have a profound effect upon the composition:
1 – download the score and place the complete folder in a suitable storage area or dedicated hard-drive.
2 – choose, or make, a 1-minute sample from a recording of a solo performance by Eric Dolphy of God Bless the Child
2 – choose, or make, a 1-minute sample of a field recording of environmental wind/ weather that takes your mind to the top of what you imagine the Junitaki Falls to be
3 - Place a clone of each of these samples into each folder in the master audio folder, in the composition folder, for example:

### Phase 2 - learning
Once you have seeded the software environment with the core audio samples, you need to teach the software by rehearsing with it. Each time you engage with the software it learns from you by memorising your response to the on-screen information. Therefore your responses will become part of its memory, and form part of the character of this version of the composition.
 
### How to perform with On Junitaki Falls A.I.
Once you have setup – see technical setup – open the performance page in the app.
In the top left-hand corner the A.I. generates a specific harmonic logic for each rehearsal iteration. Each time you reload the software this logic changes.
You can insert a user-defined duration for each rehearsal, the default is 35 minutes.
You can draw a probability over time curve to control the behavior of the A.I. performers. This can be changed during a rehearsal/ performance.
Pressing Start will initiate the process, start the visual score, arm the microphone and start collecting memories. The laptop screen will fill with the visual score, if at anytime you wish to reduce the screen size (perhaps to adjust the A.I. probability, microphone level, or the quit rehearsal) click the escape key. Click the escape key again to return it back to the large screen performance mode.
I have uploaded a demo of the generative score for
reference https://vimeo.com/164711460 . The opening is a little glitchy due to the screen capture process settling down, so in performance you will get smooth graphics, as happens after about 4 minutes through the Vimeo.

### Durations
The duration of the screen-based events (the toggle between score & black) represents breathes - a central building block in this composition. The duration of these are random, but between the defined parameters of:
Out breathe (making sound – images on screen) = 25 to 80 seconds In breathe (not making sound – black screen) = 10 to 40 seconds
The blue stripe across the bottom of the screen grows incrementally with each breath duration.

### Musical Response
There are 3 modes of response from the performer, which are linked to three specific spectral types, and you are free to choose from either of these in the here-and-now of the performance. These are note - pure tone and single pitch
node - harmonic spectra in linear or density organisation (e.g. yellow tremolo as linear, or stacked harmonics/ multi phonics as densities) noise - inharmonic spectra such as white noise (breath) or pink noise (combination of inharmonic and harmonic spectra)
   
You can choose to stick with a single spectral type through each breath, or morph between these during a breath. You are at liberty to stop during an out-breath or to circular breathe throughout.
The overall dynamic should err towards quiet, so as we enjoy the full range of spectra.
The onscreen visual language offers three main scoring modes:
1) occasional reference to the actual notes of the page, and therefore to Eric's original solo
2) graphic notation gestures
3) a visual image referencing spectral analysis tools such as sonograms

At any given moment, for any given duration a musician’s response can be positioned between literal and abstract associations to the visual and/or aural stimulus. A response can move freely across this range, it can identify with individual elements within the collective soundworld, or the duet relationship, or the fragments of visual information. It can be proactive or reactive; it can be independent or dependent. Alternatively the musician can choose to do nothing.

The visual texts can be read in their original syntax, or as abstract poetry, graphic score, libretto/ lyrics, materials for looping, cut- copy-paste or sampling and sound processing techniques. A change in the on-screen score does not necessarily dictate a change in your response.

### A poetic approach
This visual score is only a fraction of the performance. It is a trio for solo instrument and laptop, and so 2 other A.I. musicians, which the laptop generates live, will accompany you. The source material for this is you, and the machine will learn 'you' over time, and become more 'you' the more you prepare this piece. And will continue to grow over the years that you work with this piece. As such, the score will grow and become alive through you.

### Phase 3 – Ready for live performance
Once you have reach a point in the learning phase where you feel that the software is performing with you in the piece (rather than you simply teaching it), you are ready to perform the piece live. But it is important that you remove all the seed audio you embedded in Phase 1, e.g. each clone of the original Eric Dolphy sample and the wind sample.
The process for performing this piece follows the same principles and processes as the learning phase, except now the performance should now feel like a trio.

### Technical setup
1. Setup with the laptop in front of the performer.
2. A close microphone that captures the Bb instrument will need to be
connected to the laptop. This can be the internal mic of the laptop,
although this is not ideal
3. In the learning phase, the performer will find it best to wear
headphones so as to reduce bleed from the A.I. performers into the memory system of the composition. In live performance, this factor is less of an issue.
4. In the live performance AI performer 1 comes from the left audio channel, with AI performer 2 from the right. They will each need a discreet speaker positioned on stage next to the live performer as in a trio configuration.
5. Check input levels and output levels.
Stage setup
The performer will need a laptop on a heavy-duty music stand, or some other suitable stand, positioned in front of them where the paper-based score would normally be.
It is important that the stereo speakers are positioned near the live performer so that it creates a sense of a trio, with each A.I. performer projected out of their own speaker.
The visual score might also be projected behind the performer onto a large surface.
